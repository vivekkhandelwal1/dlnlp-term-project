{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Importing Libraries ##############\n",
    "import numpy as np\n",
    "import random\n",
    "import sys, os\n",
    "import json\n",
    "import argparse\n",
    "import io  \n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from gensim.models import FastText as fText\n",
    "from nltk.corpus import stopwords\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################get data\n",
    "datasets = {}\n",
    "for fname in ['train', 'dev', 'test']:\n",
    "    data = []\n",
    "    for line in open('%s/%s.res' % (dataset, fname)):\n",
    "        s = json.loads(line.strip())\n",
    "        if len(s) > 0:\n",
    "            data.append(s)\n",
    "    datasets[fname] = data\n",
    "\n",
    "    global_subwords = {}\n",
    "PREmodel = fText.load_fasttext_format('wiki.en.bin')\n",
    "fastTextVocab = PREmodel.wv.vocab\n",
    "print(\"Vocabulary built\\n\")\n",
    "stop_words = set(stopwords.words('english'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all substrings of string \n",
    "def all_substr(word):\n",
    "    res = [word[i: j] for i in range(len(word)) for j in range(i + 1, len(word) + 1)] \n",
    "    return res\n",
    "\n",
    "def all_subwords(word):            \n",
    "    subword_list = all_substr(word)\n",
    "    subWInDict = []\n",
    "    for i in subword_list:\n",
    "        if i in fastTextVocab and len(i)>3:\n",
    "            if not i in stop_words:\n",
    "                subWInDict.append(i)\n",
    "    return subWInDict\n",
    "\n",
    "wordcount = {}\n",
    "\n",
    "def dfs(node):\n",
    "    if node.__contains__('children'):\n",
    "        dfs(node['children'][0])\n",
    "        dfs(node['children'][1])\n",
    "    else:\n",
    "        word = node['word'].lower()\n",
    "        wordcount[word] = wordcount.get(word, 0) + 1\n",
    "    \n",
    "for fname in ['train', 'dev', 'test']:\n",
    "    for sent in self.origin[fname]:\n",
    "        dfs(sent)\n",
    "for ind, ite in enumerate(list(wordcount)):\n",
    "    subW = []\n",
    "    if len(ite)>4:\n",
    "        subW = all_subwords(ite)\n",
    "        for w in subW:\n",
    "            wordcount[w] = wordcount.get(w, 0) + 1\n",
    "    if len(subW)==0:\n",
    "        global_subwords[ite] = [ite]\n",
    "    else:\n",
    "        global_subwords[ite] = subW\n",
    "words = wordcount.items()\n",
    "sorted(words,key = lambda x : x[1], reverse = True)\n",
    "wordlist = {item[0]: index+1 for index, item in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Word Vectors\n",
    "dim = 300\n",
    "wordvector = []\n",
    "similarcnt = 0\n",
    "print(\"Creating word vectors\\n\")\n",
    "for i,x in enumerate(wordlist):\n",
    "    if x in fastTextVocab:\n",
    "        wordvector.append(np.array(PREmodel.wv[x]))\n",
    "    else:\n",
    "        similarcnt += 1\n",
    "        wordvector.append(np.array(PREmodel.wv[PREmodel.wv.most_similar(x)[0][0]]))\n",
    "wordvector = np.array(wordvector, dtype=np.float32)\n",
    "print (similarcnt, \"similar words used in wordvector\")\n",
    "print (len(wordvector), \"words in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "global_max = 0\n",
    "for fname in ['train', 'dev', 'test']:\n",
    "    data[fname] = []\n",
    "    for node in origin[fname]:\n",
    "        words = []\n",
    "        if node.__contains__('children'):\n",
    "            dfs(node['children'][0], words)\n",
    "            dfs(node['children'][1], words)\n",
    "        else:\n",
    "            subw = global_subwords[node['word'].lower()]\n",
    "            for wor in subw:\n",
    "                words.append(self.wordlist[wor])\n",
    "        lens = len(words)\n",
    "        if lens > global_max:\n",
    "            global_max = lens\n",
    "        if maxlength < lens:\n",
    "            print (\"******Alert****\", lens)\n",
    "        words += [0] * (maxlength - lens)\n",
    "        soultion = np.zeros(grained, dtype=np.float32)\n",
    "        solution[int(sent['rating'])] += 1.0\n",
    "        now = {'words': np.array(words), \\\n",
    "                'answer': solution,\\\n",
    "                'length': lens}\n",
    "        data[fname].append(now)\n",
    "print(\"Global Max: \", global_max )\n",
    "train_data, dev_data, test_data = data['train'], data['dev'], data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = sys.argv[1:]\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--fasttest', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=int(1000*time.time()))\n",
    "parser.add_argument('--word_vector', type=str, default='../glove-Copy1.6B.300d.txt')\n",
    "parser.add_argument('--dim', type=int, default=300)\n",
    "parser.add_argument('--tau', type=float, default=0.1)\n",
    "parser.add_argument('--dropout', type=float, default=0.5)\n",
    "parser.add_argument('--sample_cnt', type=int ,default=5)\n",
    "parser.add_argument('--LSTMpretrain', type=str, default='')\n",
    "parser.add_argument('--RLpretrain', type=str, default='')\n",
    "parser.add_argument('--maxlength', type=int, default=256)\n",
    "parser.add_argument('--grained', type=int, default=4)\n",
    "parser.add_argument('--optimizer', type=str, default='Adam')\n",
    "parser.add_argument('--alpha', type=float, default=0.1)\n",
    "parser.add_argument('--epsilon', type=float, default=0.05)\n",
    "parser.add_argument('--dataset', type=str, default='../TrainData/AG')\n",
    "parser.add_argument('--lr', type=float, default=0.0005)\n",
    "parser.add_argument('--epoch', type=int, default=3)\n",
    "parser.add_argument('--batchsize', type=int, default=64)\n",
    "args, _ = parser.parse_known_args(argv)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.fasttest == 1:\n",
    "    train_data = train_data[:5000]\n",
    "    dev_data = dev_data[:1000]\n",
    "    test_data = test_data[:1000]\n",
    "\n",
    "print (\"train_data \", len(train_data))\n",
    "print (\"dev_data\", len(dev_data))\n",
    "print (\"test_data\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous epoch loss\n",
    "prevLoss = [0.]*len(train_data)\n",
    "print(\"PrevLoss length, Type, List element type: \", len(prevLoss), type(prevLoss), type(prevLoss[0]))\n",
    "def train(sess, actor, critic, train_data, batchsize, sample=5, LSTM=True, RL=True):\n",
    "    print (\"training : total \", len(train_data))\n",
    "    for b in range(int(len(train_data) / batchsize)):\n",
    "        datas = train_data[b * batchsize: (b+1) * batchsize]\n",
    "        totalloss = 0.\n",
    "        critic.assign_active_network()\n",
    "        actor.assign_active_network()\n",
    "        for j in range(batchsize):\n",
    "            data = datas[j]\n",
    "            inputs, answer, length = data['words'], data['answer'], data['length']\n",
    "            if RL:\n",
    "                actionlist, statelist, losslist = [], [], []\n",
    "                averageloss = 0.\n",
    "                tempLoss = 0.\n",
    "                for i in range(sample):\n",
    "                    vec = critic.wordvector_find([inputs])\n",
    "                    current_lower_state = np.zeros((1, 2*args.dim), dtype=np.float32)\n",
    "                    actions = []\n",
    "                    states = []\n",
    "                    for pos in range(length):\n",
    "                        predicted = actor.predict_target(current_lower_state, [vec[0][pos]])\n",
    "                        states.append([current_lower_state, [vec[0][pos]]])\n",
    "                        if Random:\n",
    "                            if random.random() > args.epsilon:\n",
    "                                action = (0 if random.random() < predicted[0] else 1)\n",
    "                            else:\n",
    "                                action = (1 if random.random() < predicted[0] else 0)\n",
    "                        else:\n",
    "                            action = np.argmax(predicted)\n",
    "                        actions.append(action)\n",
    "                        if action == 1:\n",
    "                            out_d, current_lower_state = critic.lower_LSTM_target(current_lower_state, [[inputs[pos]]])\n",
    "\n",
    "                    Rinput = []\n",
    "                    for (i, a) in enumerate(actions):\n",
    "                        if a == 1:\n",
    "                            Rinput.append(inputs[i])\n",
    "                    Rlength = len(Rinput)\n",
    "                    if Rlength == 0:\n",
    "                        actions[length-2] = 1\n",
    "                        Rinput.append(inputs[length-2])\n",
    "                        Rlength = 1\n",
    "                    Rinput += [0] * (args.maxlength - Rlength)\n",
    "                \n",
    "                    statelist.append(states)\n",
    "                    actionlist.append(actions)\n",
    "                    out, loss = critic.getloss([Rinput], [Rlength], [answer])\n",
    "                    loss += (float(Rlength) / length) **2 *0.15\n",
    "                    averageloss += loss\n",
    "                    losslist.append(loss)\n",
    "                \n",
    "                averageloss /= sample              \n",
    "                tempLoss = averageloss - prevLoss[b * batchsize + j] \n",
    "                prevLoss[b * batchsize + j] = averageloss - (float(Rlength) / length)                    \n",
    "                aveloss = tempLoss \n",
    "                totalloss += averageloss\n",
    "                grad = None\n",
    "                if LSTM:\n",
    "                    out, loss, _ = critic.train([Rinput], [Rlength], [solution])\n",
    "                for i in range(sample):\n",
    "                    for pos in range(len(actionlist[i])):\n",
    "                        rr = [0., 0.]\n",
    "                        rr[actionlist[i][pos]] = (losslist[i] - averageloss) * args.alpha\n",
    "                        g = actor.get_gradient(statelist[i][pos][0], statelist[i][pos][1], rr)\n",
    "                        if grad == None:\n",
    "                            grad = g\n",
    "                        else:\n",
    "                            grad[0] += g[0]\n",
    "                            grad[1] += g[1]\n",
    "                            grad[2] += g[2]\n",
    "                actor.train(grad)\n",
    "                else:\n",
    "                    out, loss, _ = critic.train([inputs], [length], [answer])\n",
    "                    totalloss += loss\n",
    "        if RL:\n",
    "            actor.update_target_network()\n",
    "            if LSTM:\n",
    "                critic.update_target_network()\n",
    "        else:\n",
    "            critic.assign_target_network()\n",
    "        if (b + 1) % 200 == 0:\n",
    "            acc_test = test(sess, actor, critic, test_data, noRL= not RL)\n",
    "            acc_dev = test(sess, actor, critic, dev_data, noRL= not RL)\n",
    "            print (\"batch \",b , \"loss \", totalloss, \"test acc: \", acc_test, \", dev acc: \", acc_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure = tf.ConfigProto()\n",
    "configure.gpu_options.allow_growth = True\n",
    "with tf.Session(configure = configure) as sess:\n",
    "    critic = LSTM_CriticNetwork(sess, args.dim, args.optimizer, args.lr, args.tau, args.grained, args.maxlength, args.dropout, word_vector) \n",
    "    actor = ActorNetwork(sess, args.dim, args.optimizer, args.lr, args.tau)\n",
    "    for i in range(0, 3):\n",
    "        train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, LSTM_trainable=False)\n",
    "        acc_test = test(sess, actor, critic, test_data)\n",
    "        acc_dev = test(sess, actor, critic, dev_data)\n",
    "        print (\"RL \", i, \"test: \", acc_test, \", dev: \", acc_dev)\n",
    "    print (\"RL Model pretrained\")\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(0, 3):\n",
    "        train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt, RL_trainable=False)\n",
    "        critic.assign_target_network()\n",
    "        acc_test = test(sess, actor, critic, test_data, True)\n",
    "        acc_dev = test(sess, actor, critic, dev_data, True)\n",
    "        print (\"LSTM \",i, \"test: \", acc_test, \", dev: \", acc_dev)\n",
    "    print (\"LSTM Model pretrained\")\n",
    "    \n",
    "    for e in range(args.epoch):\n",
    "        train(sess, actor, critic, train_data, args.batchsize, args.sample_cnt)\n",
    "        acc_test = test(sess, actor, critic, test_data)\n",
    "        acc_dev = test(sess, actor, critic, dev_data)\n",
    "        print (\"epoch \", e, \"test acc: \", acc_test, \", dev acc: \", acc_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, dim, optimizer, learning_rate, tau):\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Actor\")\n",
    "        self.sess = sess\n",
    "        self.dim = dim\n",
    "        self.learning_rate = tf.train.exponential_decay(learning_rate, self.global_step, 1000, 0.95, staircase=True)\n",
    "        self.tau = tau\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.input_l, self.input_d, self.scaled_out = self.create_actor_network()\n",
    "        self.network_params = tf.trainable_variables()[self.num_other_variables:]\n",
    "        self.target_input_l, self.target_input_d, self.target_scaled_out = self.create_actor_network()\n",
    "        self.target_network_params = tf.trainable_variables()[self.num_other_variables + len(self.network_params):]\n",
    "\n",
    "        self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "            tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], \n",
    "            1 - self.tau)) for i in range(len(self.target_network_params))]        \n",
    "        self.assign_active_network_params = [self.network_params[i].assign(self.target_network_params[i]) \n",
    "                                             for i in range(len(self.network_params))]\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [2])\n",
    "        self.log_target_scaled_out = tf.log(self.target_scaled_out)\n",
    "        self.actor_gradients = tf.gradients(self.log_target_scaled_out, self.target_network_params,\n",
    "                                            self.action_gradient)\n",
    "        self.gradients = [tf.placeholder(tf.float32, [600,1]), \n",
    "                        tf.placeholder(tf.float32, [1,]),\n",
    "                        tf.placeholder(tf.float32, [300, 1])]\n",
    "        self.optimize = self.optimizer.apply_gradients(zip(self.gradients, self.network_params[:-1]), \n",
    "                                                       global_step=self.global_step)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        input_l = tf.placeholder(tf.float32, shape=[1, self.dim*2])\n",
    "        input_d = tf.placeholder(tf.float32, shape=[1, self.dim])        \n",
    "        t1 = tflearn.fully_connected(input_l, 1)\n",
    "        t2 = tflearn.fully_connected(input_d, 1)\n",
    "        scaled_out = tflearn.activation(tf.matmul(input_l,t1.W) + tf.matmul(input_d,t2.W) + t1.b, activation = 'sigmoid')        \n",
    "        s_out = tf.clip_by_value(scaled_out[0][0], 3e-4, 1 - 3e-4)\n",
    "        scaled_out = tf.stack([1.0 - s_out, s_out])\n",
    "        return input_l, input_d, scaled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Critic(object):\n",
    "    def __init__(self, sess, dim, optimizer, learning_rate, tau, grained, max_length, dropout, wordvector):\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"LSTM\")\n",
    "        self.sess = sess\n",
    "        self.max_length = max_length\n",
    "        self.dim = dim\n",
    "        self.learning_rate = tf.train.exponential_decay(learning_rate, self.global_step, 1000, 0.95, staircase=True)\n",
    "        self.tau = tau\n",
    "        self.grained = grained\n",
    "        self.dropout = dropout\n",
    "        self.init = tf.random_uniform_initializer(-0.01, 0.01, dtype=tf.float32)\n",
    "        self.L2regular = 0.0001\n",
    "        print (\"optimizer: \", optimizer)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keepprob\")\n",
    "        self.num_other_variables = len(tf.trainable_variables())\n",
    "        self.wordvector = tf.get_variable('wordvector', dtype=tf.float32, initializer=wordvector, trainable=True)\n",
    "\n",
    "        self.lower_cell_state, self.lower_cell_input, self.lower_cell_output, self.lower_cell_state1 = self.LSTM_cell('Lower/Active')\n",
    "\n",
    "        self.inputs, self.length, self.out = self.create_critic_network(\"Active\")\n",
    "        self.network_params = tf.trainable_variables()[self.num_other_variables:]        \n",
    "        self.target_wordvector = tf.get_variable('wordvector_target', dtype=tf.float32, initializer=wordvector, trainable=True)\n",
    "\n",
    "        self.target_lower_cell_state, self.target_lower_cell_input, self.target_lower_cell_output, self.target_lower_cell_state1 = self.LSTM_cell('Lower/Target')\n",
    "        \n",
    "        self.target_inputs, self.target_length, self.target_out = self.create_critic_network(\"Target\")\n",
    "        self.target_network_params = tf.trainable_variables()[len(self.network_params)+self.num_other_variables:]\n",
    "        self.update_target_network_params = \\\n",
    "                [self.target_network_params[i].assign(\\\n",
    "                tf.multiply(self.network_params[i], self.tau)+\\\n",
    "                tf.multiply(self.target_network_params[i], 1 - self.tau))\\\n",
    "                for i in range(len(self.target_network_params))]\n",
    "        \n",
    "        self.assign_target_network_params = \\\n",
    "                [self.target_network_params[i].assign(\\\n",
    "                self.network_params[i]) for i in range(len(self.target_network_params))]\n",
    "        self.assign_active_network_params = \\\n",
    "                [self.network_params[i].assign(\\\n",
    "                self.target_network_params[i]) for i in range(len(self.network_params))]\n",
    "\n",
    "        self.ground_truth = tf.placeholder(tf.float32, [1,self.grained], name=\"ground_truth\")        \n",
    "        \n",
    "        self.loss_target = tf.nn.softmax_cross_entropy_with_logits(labels=self.ground_truth, logits=self.target_out)\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.ground_truth, logits=self.out)\n",
    "        self.loss2 = 0\n",
    "        with tf.variable_scope(\"Lower/Active\", reuse=True):\n",
    "            self.loss2+= tf.nn.l2_loss(tf.get_variable('lstm_cell/kernel'))\n",
    "        with tf.variable_scope(\"Active/pred\", reuse=True):\n",
    "            self.loss2+= tf.nn.l2_loss(tf.get_variable('W'))\n",
    "        self.loss += self.loss2 * self.L2regular\n",
    "        self.loss_target += self.loss2 * self.L2regular\n",
    "        self.gradients = tf.gradients(self.loss_target, self.target_network_params)\n",
    "        self.optimize = self.optimizer.apply_gradients(zip(self.gradients, self.network_params), global_step = self.global_step)\n",
    "        \n",
    "        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "        self.WVinput, self.WVvec = self.create_wordvector_find()\n",
    "\n",
    "    def create_critic_network(self, Scope):\n",
    "        inputs = tf.placeholder(shape=[1, self.max_length], dtype=tf.int32, name=\"inputs\")\n",
    "        length = tf.placeholder(shape=[1], dtype=tf.int32, name=\"length\")\n",
    "       \n",
    "        if Scope[-1] == 'e':\n",
    "            vec = tf.nn.embedding_lookup(self.wordvector, inputs)\n",
    "        else:\n",
    "            vec = tf.nn.embedding_lookup(self.target_wordvector, inputs)\n",
    "        cell = LSTMCell(self.dim, initializer=self.init, state_is_tuple=False)\n",
    "        \n",
    "        with tf.variable_scope(\"Lower\", reuse=True):\n",
    "            out, _ = tf.nn.dynamic_rnn(cell, vec, length, dtype=tf.float32, scope=Scope)\n",
    "        out = tf.gather(out[0], length-1)\n",
    "        \n",
    "        out = tflearn.dropout(out, self.keep_prob)\n",
    "        out = tflearn.fully_connected(out, self.grained, scope=Scope+\"/pred\", name=\"pred\")\n",
    "        return inputs, length, out\n",
    "    \n",
    "    def LSTM_cell(self,Scope):\n",
    "        cell = LSTMCell(self.dim, initializer=self.init, state_is_tuple=False)\n",
    "        state = tf.placeholder(tf.float32, shape = [1, cell.state_size], name=\"state\")\n",
    "        inputs = tf.placeholder(tf.int32, shape = [1, 1], name=\"input\")\n",
    "        if Scope[-1] == 'e':\n",
    "            vec = tf.nn.embedding_lookup(self.wordvector, inputs)\n",
    "        else:\n",
    "            vec = tf.nn.embedding_lookup(self.target_wordvector, inputs)\n",
    "        with tf.variable_scope(Scope, reuse=False):\n",
    "            out, state1 = cell(vec[:,0,:], state)\n",
    "        return state, inputs, out, state1\n",
    "\n",
    "    def getloss(self, inputs, length, ground_truth):\n",
    "        return self.sess.run([self.target_out, self.loss_target], feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_length: length,\n",
    "            self.ground_truth: ground_truth,\n",
    "            self.keep_prob: 1.0})\n",
    "\n",
    "    def train(self, inputs, length, ground_truth):\n",
    "        return self.sess.run([self.target_out, self.loss_target, self.optimize], feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_length: length,\n",
    "            self.ground_truth: ground_truth,\n",
    "            self.keep_prob: self.dropout})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
